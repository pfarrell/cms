---
date: '2024-02-23'
headerImage: null
id: 1708704210
image: null
kebab: notes-from-google-machine-learning-crash-course
projectLink: null
summary: Write a summary
title: Notes from Google Machine Learning Crash Course
---

These are notes from [Google's machine learning crash course](https://developers.google.com/machine-learning/crash-course)


## Intro
* Steps in developing a model
  * Data acquisision
  * Data cleaning/understanding
  * Choose a model
  * Feature Engineering
  * Training
    * Calculate Loss (RMSE
    * Learning Rate
    * Regularization (L<sub>1</sub>, L<sub>2</sub>)
    * Visualization (matplotlib)

## Logistic Regression
  * Linear Regression can produce models that produce unrealistic results when 
  * Logistic Regression produces probabilities that are calibrated
  * Uses Sigmoids (i.e. tanh) 
  * Regulariztion helps penalizing large weights and allowing early stopping
  * Linear logistic regression is efficient and scales to large datasets
    * nonlineararity can be expressed using feature cross products
  * Loss function for linear regression is squared loss, Loss function for logistic regression is Log loss
  * Logistic regression without regularization would drive loss to 0 in higher dimensions, L<sub>2</sub> regularization **prevents** over fitting and reduces model complexity*

## Classification
* Classification can be based on logistic regression
    * i.e. spam if classification threshold is exceeded

### Confusion Matrix

  || Actually True | Actually False |
  |---|---|---|
  |Predict True| True Positive | False Positive |
  | Prediction False| False Negative | True Negative |


### Accuracy
* Accuracy measures model performance
* **Accuracy**: Correct predictions / Total predictions
    * _(True Positive + True Negative) / (True Positive + True Negative + False Positive + False Negative)_
    * Accuracy doesn't tell the full story when working with a class-imbalanced data set (where big difference between number of classifications e.g. an ad was clicked where 99.9% of time ads aren't clicked
    * Precision and Recall help in class-imbalance data sets
* **Precision**: _True Positives / (True Positive + False Positive)_
    * What proportion of positive identifications are actually correct?
    * did the model make false Positives often?
* **Recall**: _True Positives / (True Positive + False Negative)_
    * What proportion of actual positives were identified correctly
    * did the have false negatives?
    * Recall and precision go hand in hand in determining model efficacy
    * we may not know in advance what classification threshold to use.
* a **ROC curve** (Receiver Operating Characteristics) evaluates all classifications threshold and evaluates the precision and recal for them, then draws a graph with true positive versus false positive rates
      * Curve has two params: **True Positive Rate** (aka Recall), **False Positive Rate**: _False Positive / (False Positive + True Negative)_
* an **AUC curve** (Area Under the ROC Curve)
    * measures the area under a ROC curve integrating from 0 to 1 for the Classification Thresholds
    * AUC is scale-invariant. It measures how well predictions are ranked, rather than their absolute values.
    * AUC is classification-threshold-invariant. It measures the quality of the model's predictions irrespective of what classification threshold is chosen.
    * Scale invariance is not always desirable. For example, sometimes we really do need well calibrated probability outputs, and AUC wonâ€™t tell us about that.
    * Classification-threshold invariance is not always desirable. In cases where there are wide disparities in the cost of false negatives vs. false positives, it may be critical to minimize one type of classification error. For example, when doing email spam detection, you likely want to prioritize minimizing false positives (even if that results in a significant increase of false negatives). AUC isn't a useful metric for this type of optimization.
* **Prediction Bias**: a comparison of the average of predictions versus average of observations
    * bias of 0 shows predictions == observations
    * if you have bias, you have a problem in the model
    * zero bias is not a measure of a good model
    * calibration plots show bucketed bias

## Regularization for Sparsity

* Feature Crosses with high cardinality data can make a model not work
* When crossing sparse features (i.e. search terms X videos): noisy coefficients
* Need to regularize and zero out weights that are noise/rare

### L<sub>1</sub> Regularization

* For models with lots of sparse featuers, we would like for as many weights as possible to go to 0
* L<sub>2</sub> Regularization minimizes the square of the weights thus removing a percentage of impact, never going to 0 completely
* L<sub>0</sub> Regularization would penalize a model for for having non-zero weight coefficients, but this approach leads to non-convex optimization which is np-hard and to be avoided if possible
* L<sub>1</sub> Regularization is convex and gets weights to exactly 0

#### L<sub>1</sub> versus L<sub>2</sub>

* L<sub>2</sub> penalizes _weight_<sup>2</sup> and has a derivative of _2 * weight_
* L<sub>1</sub> penalizes |_weight_| and has a derivative of _k_ (independent of weight)
* L<sub>2</sub> removes a percentage of weight while L1 removes a contant amount of weight each time

## Neural Networks

* Neural nets are good for very non-linear problems (i.e. feature cross products don't work well)
* Models should learn the non-linearities without us having to specify them
* Adding hidden layers doesn't produce non-linearity
* We need to add a non-linear transforms aka **activation functions**
* **Relu** (Rectified Linear Unit) is a simple non-linear function: _F(x) = max(0,x)_
  * **Sigmoid** (e.g. _F(x) = 1 / (1 + e<sup>-x</sup>)_ ) and **_tanh_** are other non-linear functions that are common
* *Gradient Descent* is replaced by **Back Propogation** in neural networks
* another way to say it: _nonlinear_ means you can't predict a label with a model of the form _b + w<sub>1</sub>x<sub>1</sub> + w<sub>2</sub>x<sub>2</sub>_

### Standard components of a neural network
* set of layers of nodes (neurons)
* set of weights representing connection between each nn layer
* set of biases for each node
* an activation function for each layer

## Best Practices for Training NNs

* **Vanishing Gradients**: gradients for lower layers can become small and cause a lot of calculation that doesn't affect the model.  Getting gradients to zero using a Relu activation fuction is one method of preventing them
* **Exploding Gradients**: opposite of vanishing gradients, where gradients become large and don't converge.  Batch normalization and lowerering the learning rate can prevent exploding gradients
* **Dead Relu Units**: Once Relu falls below 0, it never comes back.  Lowering the learning rate can help keep Relu from dying prematurely
* **Dropout Regularization** is removing a node with probability _P_ during a gradient step to determine if it's presense is having a negative affect on the model

## Multi-class neural networks
* From exercise on MNIST images to recognize handwritten digits
* A nn with 256 nodes then a dropout of .4 produced 98% accuracy on the task

## Embeddings
* Embeddings are a way to represent data as a vector in many dimensions
* Embeddings are motivated from collaborative filtering where we want to use user behavior to determine similarity and clustering for recommendations
* **categorical input data** refers to input features the represent one or more discrete items from a finite set of choices (i.e. movies watched from all movies, music track played from all tracks)
* We use sparse tensors to represent the wide range of data available (e.g. list of ids of movies watched rather than a one-hot encoding of 500k movies)
* huge input vectors create challenges
  * very huge number of weights in the nn (i.e. M words and N nodes results in _M x N_ weights
  * results in large amount of data and computation
* **embeddings** are the solution to sparse data translating large sparse vectors into lower-dimensional space while preserving semantic relationships
* useful embeddings are on the order of hundreds of dimensions (likely several orders of magnitude smaller than the source data)

## ML Engineering
* Full range of an ML application/system includes
  * Data collection
  * Data Verification
  * Machine Resource Management
  * Analysis Tooling
  * Feature Extraction
  * Process Management Tools
  * Configuration
  * Monitoring
  * Serving Infrastructure
### Static versus Dynamic training

* A static model is trained offline, deployed to production, and monitored
* A dynamic model is trained online, continually changing as new data enters the system

### Static versus Dynamic Inference

* Static/Offline Inference requires all data to be present at model creation time, costs are predictable, and can verify the predictions before production release 
* Dynamic/Online Inference introduces latency costs, monitoring, output distribution variance, but handles new data without retraining 

### Data dependencies

* features take the place of code in ML systems, how do they get verified?
* Is the data reliably available?
* Is the feature going to change and we need to version it?
* Does the usefullness of the signal justify the cost?
* Is the data correlated, causal, or a proxy for other features?
* Which of my input signals may be impacted by the model's output (e.g. two stock market prediction models may be entangled and behavior in one is learned into the other, like the Anne Hathaway/Berkshire Hathaway stock bug)

### Fairness
* Wikiepedia [lists over 100 types of cognitive bias](https://wikipedia.org/wiki/List_of_cognitive_biases)
* Common Biases in data
  * Reporting Bias: actual frequency of events in real world not reflected in the data because they "go without saying" or are prototypical
  * Selection Bias: when a dataset's examples are chosen in a way not reflecting the real world
  * Overgeneralization
  * Out-group homogeneity bias: in-groups are described in more detail than out-groups which are stereotyped in data collection
  * Confirmation Bias
  * Automation Bias: favoring results generated by automated systems
  * Unconscious Bias: allowing one's mental models to affect data collection/interpreation/inclusion

* Evaluating Bias
  * missing features have responses in a dataset.  E.g. in the california housing dataset, 82% of population, households, and median_income feautres didn't have data
  * unexpected feature values can indicate data collection issues or other biases
  * Data skew e.g. california housing dataset included only northern california data, would be unacceptable to apply models generated from it to SoCal
  * Precision and Recall should be calculated for subgroups within a model's output, not just for entire dataset. e.g. example of cancer screening that was very effective for female and not effective for males, but when taken together looked pretty good
